# GridWorld RL avec Stable-Baselines3

Projet d'apprentissage par renforcement sur un environnement GridWorld configurable avec Stable-Baselines3 et RL-Zoo3.

## üìã Table des mati√®res

- [Installation](#installation)
- [Structure du projet](#structure-du-projet)
- [Environnements disponibles](#environnements-disponibles)
- [Utilisation rapide](#utilisation-rapide)
- [Entra√Ænement](#entra√Ænement)
- [√âvaluation](#√©valuation)
- [Visualisation](#visualisation)
- [Param√®tres des algorithmes](#param√®tres-des-algorithmes)
- [R√©sultats attendus](#r√©sultats-attendus)

## üöÄ Installation

### Pr√©requis

- Python 3.8+
- pip

### Installation des d√©pendances

```bash
# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Contenu de requirements.txt

```
gymnasium>=0.29.0
stable-baselines3>=2.0.0
rl-zoo3>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
tensorboard>=2.13.0
pygame>=2.5.0
pandas>=2.0.0
seaborn>=0.12.0
```

## üìÅ Structure du projet

```
gridworld_rl/
‚îú‚îÄ‚îÄ gridworld_env/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Enregistrement des environnements
‚îÇ   ‚îî‚îÄ‚îÄ gridworld.py          # Environnement GridWorld
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ ppo_gridworld.yml     # Configuration PPO
‚îÇ   ‚îî‚îÄ‚îÄ dqn_gridworld.yml     # Configuration DQN
‚îú‚îÄ‚îÄ train.py                  # Script d'entra√Ænement
‚îú‚îÄ‚îÄ evaluate.py               # Script d'√©valuation
‚îú‚îÄ‚îÄ visualize_training.py     # Script de visualisation
‚îú‚îÄ‚îÄ run_all.py                # Pipeline complet
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances
‚îî‚îÄ‚îÄ README.md                 # Ce fichier
```

## üéÆ Environnements disponibles

### 1. GridWorld-Simple-v0
- Grille 5√ó5
- Goal fixe en bas √† droite
- 3 obstacles fixes
- **Id√©al pour d√©buter**

### 2. GridWorld-MovingGoals-v0
- Grille 8√ó8
- Goal mobile (probabilit√© 30%)
- 4 obstacles fixes
- **Difficult√© moyenne**

### 3. GridWorld-MovingObstacles-v0
- Grille 10√ó10
- Goal fixe
- 5 obstacles mobiles (probabilit√© 20%)
- **Difficult√© moyenne-√©lev√©e**

### 4. GridWorld-FullDynamic-v0
- Grille 10√ó10
- 2 goals mobiles
- 6 obstacles mobiles (probabilit√© 25%)
- **Difficult√© maximale**

## ‚ö° Utilisation rapide

### Pipeline complet (recommand√©)

```bash
# Entra√Æner PPO sur grille simple + √©valuer + visualiser
python run_all.py --algo ppo --env GridWorld-Simple-v0 --all

# Entra√Æner DQN sur goals mobiles (200k steps)
python run_all.py --algo dqn --env GridWorld-MovingGoals-v0 --timesteps 200000 --all

# Environnement difficile avec PPO (8 envs parall√®les)
python run_all.py --algo ppo --env GridWorld-FullDynamic-v0 --timesteps 300000 --n_envs 8 --all
```

## üéØ Entra√Ænement

### Commande de base

```bash
python train.py --algo ppo --env GridWorld-Simple-v0 --timesteps 100000
```

### Options disponibles

```bash
python train.py \
    --algo ppo \                      # Algorithme: ppo ou dqn
    --env GridWorld-Simple-v0 \       # Environnement
    --timesteps 100000 \              # Nombre de steps
    --n_envs 4 \                      # Envs parall√®les (PPO uniquement)
    --save_path ./models              # Dossier de sauvegarde
```

### Exemples d'entra√Ænement

```bash
# PPO sur grille simple (rapide)
python train.py --algo ppo --env GridWorld-Simple-v0 --timesteps 100000

# DQN sur goals mobiles
python train.py --algo dqn --env GridWorld-MovingGoals-v0 --timesteps 200000

# PPO sur environnement complexe (plus long)
python train.py --algo ppo --env GridWorld-FullDynamic-v0 --timesteps 300000 --n_envs 8
```

### Suivre l'entra√Ænement avec TensorBoard

```bash
tensorboard --logdir ./models/logs
```

Ouvrir dans le navigateur: `http://localhost:6006`

## üìä √âvaluation

### Commande de base

```bash
python evaluate.py \
    --model ./models/ppo_GridWorld-Simple-v0_final \
    --algo ppo \
    --env GridWorld-Simple-v0 \
    --episodes 10
```

### Options disponibles

```bash
python evaluate.py \
    --model ./models/ppo_GridWorld-Simple-v0_final \  # Chemin du mod√®le
    --algo ppo \                                       # Algorithme utilis√©
    --env GridWorld-Simple-v0 \                        # Environnement
    --episodes 10 \                                    # Nombre d'√©pisodes
    --delay 0.3 \                                      # D√©lai entre steps (sec)
    --no_render                                        # Pas d'affichage console
```

### Exemples d'√©valuation

```bash
# √âvaluation standard
python evaluate.py --model ./models/ppo_GridWorld-Simple-v0_final --algo ppo --env GridWorld-Simple-v0

# √âvaluation d√©taill√©e (20 √©pisodes, ralenti)
python evaluate.py --model ./models/best/best_model --algo ppo --env GridWorld-Simple-v0 --episodes 20 --delay 0.5

# √âvaluation rapide sans affichage
python evaluate.py --model ./models/dqn_GridWorld-MovingGoals-v0_final --algo dqn --env GridWorld-MovingGoals-v0 --no_render --episodes 50
```

### Interpr√©tation des r√©sultats

L'√©valuation affiche:
- **Taux de succ√®s**: % d'√©pisodes o√π le goal est atteint
- **R√©compense moyenne**: Performance globale
- **Longueur moyenne**: Nombre de steps par √©pisode

**Bon r√©sultat**: Taux de succ√®s > 80%, r√©compense positive, longueur minimale

## üìà Visualisation

### Cr√©er les graphiques

```bash
python visualize_training.py --log_dir ./models/logs --save_path ./plots
```

### Options disponibles

```bash
python visualize_training.py \
    --log_dir ./models/logs \    # Dossier des logs TensorBoard
    --save_path ./plots \        # Dossier de sortie
    --smooth 0.9                 # Lissage (0-1)
```

### Graphiques g√©n√©r√©s

1. **training_curves.png**: Vue d'ensemble (r√©compense, longueur, loss, learning rate)
2. **reward_evolution.png**: √âvolution d√©taill√©e de la r√©compense
3. **algorithm_comparison.png**: Comparaison entre algorithmes (si applicable)
4. **performance_summary.png**: R√©sum√© visuel des performances

## üîß Param√®tres des algorithmes

### PPO (Proximal Policy Optimization)

**Inspir√©s de FrozenLake-v1 et adapt√©s √† GridWorld**

```python
{
    'learning_rate': 0.0003,
    'n_steps': 2048,         # Steps par environnement avant update
    'batch_size': 64,        # Taille des mini-batchs
    'n_epochs': 10,          # Epochs d'optimisation
    'gamma': 0.99,           # Facteur de discount
    'gae_lambda': 0.95,      # GAE lambda
    'clip_range': 0.2,       # Clip range PPO
    'ent_coef': 0.01,        # Coefficient d'entropie (exploration)
    'vf_coef': 0.5,          # Coefficient value function
    'max_grad_norm': 0.5     # Gradient clipping
}
```

**Pourquoi ces valeurs?**
- `n_steps=2048`: √âquilibre entre variance et biais
- `ent_coef=0.01`: Encourage l'exploration dans un espace discret
- `clip_range=0.2`: Valeur standard √©prouv√©e

### DQN (Deep Q-Network)

**Inspir√©s de FrozenLake-v1 et CartPole-v1**

```python
{
    'learning_rate': 0.0001,
    'buffer_size': 100000,            # Taille du replay buffer
    'learning_starts': 1000,          # Steps avant apprentissage
    'batch_size': 32,                 # Taille des mini-batchs
    'tau': 1.0,                       # Hard update target network
    'gamma': 0.99,                    # Facteur de discount
    'train_freq': 4,                  # Fr√©quence d'entra√Ænement
    'gradient_steps': 1,              # Steps de gradient par update
    'target_update_interval': 1000,   # Fr√©quence update target
    'exploration_fraction': 0.1,      # Fraction pour epsilon decay
    'exploration_initial_eps': 1.0,   # Epsilon initial
    'exploration_final_eps': 0.05     # Epsilon final
}
```

**Pourquoi ces valeurs?**
- `buffer_size=100000`: Grande m√©moire pour diversit√©
- `exploration_fraction=0.1`: Exploration rapide puis exploitation
- `target_update_interval=1000`: Stabilit√© de l'apprentissage

### Ajustements pour environnements dynamiques

Pour **MovingGoals** et **MovingObstacles**:
- Augmenter `ent_coef` (PPO): 0.02-0.03 ‚Üí Plus d'exploration
- Augmenter `exploration_fraction` (DQN): 0.2-0.25 ‚Üí Explorer plus longtemps
- Augmenter `n_timesteps`: 200k-300k ‚Üí Plus de donn√©es

## üìà R√©sultats attendus

### GridWorld-Simple-v0

| Algo | Timesteps | Taux succ√®s | R√©compense moy. | Longueur moy. |
|------|-----------|-------------|-----------------|---------------|
| PPO  | 100k      | 85-95%      | 8-9             | 8-10          |
| DQN  | 100k      | 80-90%      | 7-8             | 9-11          |

### GridWorld-MovingGoals-v0

| Algo | Timesteps | Taux succ√®s | R√©compense moy. | Longueur moy. |
|------|-----------|-------------|-----------------|---------------|
| PPO  | 200k      | 70-85%      | 6-8             | 12-16         |
| DQN  | 200k      | 65-80%      | 5-7             | 14-18         |

### GridWorld-FullDynamic-v0

| Algo | Timesteps | Taux succ√®s | R√©compense moy. | Longueur moy. |
|------|-----------|-------------|-----------------|---------------|
| PPO  | 300k      | 60-75%      | 4-6             | 18-25         |
| DQN  | 300k      | 55-70%      | 3-5             | 20-28         |

**Note**: Les r√©sultats varient selon le seed et la configuration exacte.

## üéì Exemples complets

### Exemple 1: D√©butant (Simple)

```bash
# Entra√Æner PPO pendant 100k steps
python run_all.py --algo ppo --env GridWorld-Simple-v0 --timesteps 100000 --all

# R√©sultats attendus: ~90% succ√®s en 8-10 steps
```

### Exemple 2: Interm√©diaire (Goals mobiles)

```bash
# Entra√Æner DQN pendant 200k steps
python run_all.py --algo dqn --env GridWorld-MovingGoals-v0 --timesteps 200000 --all

# R√©sultats attendus: ~75% succ√®s en 14-16 steps
```

### Exemple 3: Avanc√© (Environnement dynamique)

```bash
# Entra√Æner PPO avec 8 environnements parall√®les
python run_all.py --algo ppo --env GridWorld-FullDynamic-v0 --timesteps 300000 --n_envs 8 --all

# R√©sultats attendus: ~65% succ√®s en 20-25 steps
```

### Exemple 4: Comparaison d'algorithmes

```bash
# Entra√Æner PPO
python train.py --algo ppo --env GridWorld-Simple-v0 --timesteps 100000

# Entra√Æner DQN
python train.py --algo dqn --env GridWorld-Simple-v0 --timesteps 100000

# √âvaluer les deux
python evaluate.py --model ./models/ppo_GridWorld-Simple-v0_final --algo ppo --env GridWorld-Simple-v0 --episodes 20
python evaluate.py --model ./models/dqn_GridWorld-Simple-v0_final --algo dqn --env GridWorld-Simple-v0 --episodes 20

# Visualiser et comparer
python visualize_training.py
```

## üêõ D√©pannage

### Probl√®me: "Module 'gridworld_env' not found"

```bash
# Ajouter le dossier au PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Linux/Mac
set PYTHONPATH=%PYTHONPATH%;%CD%          # Windows
```

### Probl√®me: Agent ne converge pas

- Augmenter `n_timesteps` (doubler la dur√©e)
- Augmenter `ent_coef` (PPO) ou `exploration_fraction` (DQN)
- V√©rifier que l'environnement est solvable (pas trop d'obstacles)

### Probl√®me: Entra√Ænement tr√®s lent

- R√©duire `n_envs` (PPO) ou `batch_size`
- Utiliser un GPU si disponible
- R√©duire la taille de la grille

## üìö R√©f√©rences

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [RL-Zoo3](https://github.com/DLR-RM/rl-baselines3-zoo)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DQN Paper](https://arxiv.org/abs/1312.5602)

## üìù Licence

Projet acad√©mique - Libre d'utilisation pour fins √©ducatives.

---

**Bon apprentissage! üöÄ**